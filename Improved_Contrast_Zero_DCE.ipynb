{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T04:42:42.635988Z","iopub.status.busy":"2022-01-27T04:42:42.635555Z","iopub.status.idle":"2022-01-27T04:42:42.659003Z","shell.execute_reply":"2022-01-27T04:42:42.658401Z","shell.execute_reply.started":"2022-01-27T04:42:42.635898Z"},"executionInfo":{"elapsed":24171,"status":"ok","timestamp":1641842767135,"user":{"displayName":"Nandi Soumyadeep Samir Anindita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03355181599110857206"},"user_tz":-330},"id":"SzynhV16lO7o","outputId":"0da19423-080b-4433-a35b-5000649e81b0","trusted":true},"outputs":[],"source":["# Ran On kaggle using Nvidia p100 for 200 epochs\n","# Results with code are in other ipynb file , this is just for viewing code\n","# For Viewing Results download the 'Improved_Contrast_Zero_DCE_with_Results.ipynb' file as this large file will not render in github"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-01-27T04:42:48.089967Z","iopub.status.busy":"2022-01-27T04:42:48.089351Z","iopub.status.idle":"2022-01-27T04:42:51.340996Z","shell.execute_reply":"2022-01-27T04:42:51.340333Z","shell.execute_reply.started":"2022-01-27T04:42:48.089927Z"},"executionInfo":{"elapsed":14449,"status":"ok","timestamp":1641842784297,"user":{"displayName":"Nandi Soumyadeep Samir Anindita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03355181599110857206"},"user_tz":-330},"id":"1daab143","outputId":"45b632be-c1a5-42a3-89a2-174c5a1a7c66","trusted":true},"outputs":[],"source":["#Dataloader\n","import os\n","import sys\n","\n","import torch\n","import torch.utils.data as data\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import glob\n","import random\n","import matplotlib.pyplot as plt\n","\n","\n","random.seed(1143)\n","\n","lowlight_images_path = '../input/low-light/data/train_data/'\n","\n","def populate_train_list(lowlight_images_path):\n","\n","\n","\timage_list_lowlight = glob.glob(lowlight_images_path + \"*.jpg\")\n","\n","\ttrain_list = image_list_lowlight\n","\n","\trandom.shuffle(train_list)\n","\n","\treturn train_list\n","\n","\t\n","\n","class lowlight_loader(data.Dataset):\n","\n","\tdef __init__(self, lowlight_images_path):\n","\n","\t\tself.train_list = populate_train_list(lowlight_images_path) \n","\t\tself.size = 512\n","\n","\t\tself.data_list = self.train_list\n","\t\tprint(\"Total training examples:\", len(self.train_list))\n","\n","\n","\t\t\n","\n","\tdef __getitem__(self, index):\n","\n","\t\tdata_lowlight_path = self.data_list[index]\n","\t\t\n","\t\tdata_lowlight = Image.open(data_lowlight_path)\n","\t\t\n","\t\tdata_lowlight = data_lowlight.resize((self.size,self.size), Image.ANTIALIAS)\n","\n","\t\tdata_lowlight = (np.asarray(data_lowlight)/255.0) \n","\t\tdata_lowlight = torch.from_numpy(data_lowlight).float()\n","\n","\t\treturn data_lowlight.permute(2,0,1)\n","\n","\tdef __len__(self):\n","\t\treturn len(self.data_list)\n","\t\n","\n","x = lowlight_loader(lowlight_images_path)\n","y = x.__getitem__(9).permute(1,2,0).numpy()\n","print(y.shape)\n","plt.imshow(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-26T08:28:49.516456Z","iopub.status.busy":"2022-01-26T08:28:49.51592Z","iopub.status.idle":"2022-01-26T08:28:49.541244Z","shell.execute_reply":"2022-01-26T08:28:49.540141Z","shell.execute_reply.started":"2022-01-26T08:28:49.516407Z"},"executionInfo":{"elapsed":353,"status":"ok","timestamp":1641842792742,"user":{"displayName":"Nandi Soumyadeep Samir Anindita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03355181599110857206"},"user_tz":-330},"id":"35620c12","trusted":true},"outputs":[],"source":["#Model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","#import pytorch_colors as colors\n","import numpy as np\n","\n","class enhance_net_nopool(nn.Module):\n","\n","\tdef __init__(self):\n","\t\tsuper(enhance_net_nopool, self).__init__()\n","\n","\t\tself.relu = nn.ReLU(inplace=True)\n","\n","\t\tnumber_f = 32\n","\t\tself.e_conv1 = nn.Conv2d(3,number_f,3,1,1,bias=True) # batch_size*number_f*\n","\t\tself.e_conv2 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n","\t\tself.e_conv3 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n","\t\tself.e_conv4 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n","\t\tself.e_conv5 = nn.Conv2d(number_f*2,number_f,3,1,1,bias=True) \n","\t\tself.e_conv6 = nn.Conv2d(number_f*2,number_f,3,1,1,bias=True) \n","\t\tself.e_conv7 = nn.Conv2d(number_f*2,24,3,1,1,bias=True) \n","\n","\t\tself.maxpool = nn.MaxPool2d(2, stride=2, return_indices=False, ceil_mode=False)\n","\t\tself.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n","\n","\n","\t\t\n","\tdef forward(self, x):\n","\t\tnp_x = np.array(x.permute(0,2,3,1).cpu())\n","\t\tfor i in range(np_x.shape[0]):       \n","\t\t\ttemp = np_x[i]\n","\t\t\tclahe = cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))\n","\t\t\ttemp[:,:,0] = clahe.apply((temp[:,:,0]*255).astype('uint8'))\n","\t\t\ttemp[:,:,1] = clahe.apply((temp[:,:,1]*255).astype('uint8'))\n","\t\t\ttemp[:,:,2] = clahe.apply((temp[:,:,2]*255).astype('uint8'))\n","\t\t\tnp_x[i] = temp/255.0\n","\t\tx = torch.from_numpy(np_x).cuda().float()\n","\t\tx = x.permute(0,3,1,2)\n","\t\tx1 = self.relu(self.e_conv1(x))\n","\t\t# p1 = self.maxpool(x1)\n","\t\tx2 = self.relu(self.e_conv2(x1))\n","\t\t# p2 = self.maxpool(x2)\n","\t\tx3 = self.relu(self.e_conv3(x2))\n","\t\t# p3 = self.maxpool(x3)\n","\t\tx4 = self.relu(self.e_conv4(x3))\n","\n","\t\tx5 = self.relu(self.e_conv5(torch.cat([x3,x4],1)))\n","\t\t# x5 = self.upsample(x5)\n","\t\tx6 = self.relu(self.e_conv6(torch.cat([x2,x5],1)))\n","\n","\t\tx_r = torch.tanh(self.e_conv7(torch.cat([x1,x6],1)))\n","\t\tr1,r2,r3,r4,r5,r6,r7,r8 = torch.split(x_r, 3, dim=1)\n","\n","\n","\t\tx = x + r1*(torch.pow(x,2)-x)\n","\t\tx = x + r2*(torch.pow(x,2)-x)\n","\t\tx = x + r3*(torch.pow(x,2)-x)\n","\t\tenhance_image_1 = x + r4*(torch.pow(x,2)-x)\t\t\n","\t\tx = enhance_image_1 + r5*(torch.pow(enhance_image_1,2)-enhance_image_1)\t\t\n","\t\tx = x + r6*(torch.pow(x,2)-x)\t\n","\t\tx = x + r7*(torch.pow(x,2)-x)\n","\t\tenhance_image = x + r8*(torch.pow(x,2)-x)\n","\t\tr = torch.cat([r1,r2,r3,r4,r5,r6,r7,r8],1)\n","\t\treturn enhance_image_1,enhance_image,r\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-26T08:28:49.543885Z","iopub.status.busy":"2022-01-26T08:28:49.543275Z","iopub.status.idle":"2022-01-26T08:28:49.59044Z","shell.execute_reply":"2022-01-26T08:28:49.589356Z","shell.execute_reply.started":"2022-01-26T08:28:49.543836Z"},"executionInfo":{"elapsed":1256,"status":"ok","timestamp":1641842801206,"user":{"displayName":"Nandi Soumyadeep Samir Anindita","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03355181599110857206"},"user_tz":-330},"id":"47667f03","trusted":true},"outputs":[],"source":["#Loss\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","from torchvision.models.vgg import vgg16\n","import numpy as np\n","\n","\n","class L_color(nn.Module):\n","\n","    def __init__(self):\n","        super(L_color, self).__init__()\n","\n","    def forward(self, x):\n","\n","        b,c,h,w = x.shape\n","\n","        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n","        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n","        Drg = torch.pow(mr-mg,2)\n","        Drb = torch.pow(mr-mb,2)\n","        Dgb = torch.pow(mb-mg,2)\n","        k = torch.pow(torch.pow(Drg,2) + torch.pow(Drb,2) + torch.pow(Dgb,2),0.5)\n","\n","\n","        return k\n","\n","\t\t\t\n","class L_spa(nn.Module):\n","\n","    def __init__(self):\n","        super(L_spa, self).__init__()\n","        # print(1)kernel = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n","        kernel_left = torch.FloatTensor( [[0,0,0],[-1,1,0],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n","        kernel_right = torch.FloatTensor( [[0,0,0],[0,1,-1],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n","        kernel_up = torch.FloatTensor( [[0,-1,0],[0,1,0 ],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n","        kernel_down = torch.FloatTensor( [[0,0,0],[0,1,0],[0,-1,0]]).cuda().unsqueeze(0).unsqueeze(0)\n","        self.weight_left = nn.Parameter(data=kernel_left, requires_grad=False)\n","        self.weight_right = nn.Parameter(data=kernel_right, requires_grad=False)\n","        self.weight_up = nn.Parameter(data=kernel_up, requires_grad=False)\n","        self.weight_down = nn.Parameter(data=kernel_down, requires_grad=False)\n","        self.pool = nn.AvgPool2d(4)\n","    def forward(self, org , enhance ):\n","        b,c,h,w = org.shape\n","\n","        org_mean = torch.mean(org,1,keepdim=True)\n","        enhance_mean = torch.mean(enhance,1,keepdim=True)\n","\n","        org_pool =  self.pool(org_mean)\t\t\t\n","        enhance_pool = self.pool(enhance_mean)\t\n","\n","        weight_diff =torch.max(torch.FloatTensor([1]).cuda() + 10000*torch.min(org_pool - torch.FloatTensor([0.3]).cuda(),torch.FloatTensor([0]).cuda()),torch.FloatTensor([0.5]).cuda())\n","        E_1 = torch.mul(torch.sign(enhance_pool - torch.FloatTensor([0.5]).cuda()) ,enhance_pool-org_pool)\n","\n","\n","        D_org_letf = F.conv2d(org_pool , self.weight_left, padding=1)\n","        D_org_right = F.conv2d(org_pool , self.weight_right, padding=1)\n","        D_org_up = F.conv2d(org_pool , self.weight_up, padding=1)\n","        D_org_down = F.conv2d(org_pool , self.weight_down, padding=1)\n","\n","        D_enhance_letf = F.conv2d(enhance_pool , self.weight_left, padding=1)\n","        D_enhance_right = F.conv2d(enhance_pool , self.weight_right, padding=1)\n","        D_enhance_up = F.conv2d(enhance_pool , self.weight_up, padding=1)\n","        D_enhance_down = F.conv2d(enhance_pool , self.weight_down, padding=1)\n","\n","        D_left = torch.pow(D_org_letf - D_enhance_letf,2)\n","        D_right = torch.pow(D_org_right - D_enhance_right,2)\n","        D_up = torch.pow(D_org_up - D_enhance_up,2)\n","        D_down = torch.pow(D_org_down - D_enhance_down,2)\n","        E = (D_left + D_right + D_up +D_down)\n","        # E = 25*(D_left + D_right + D_up +D_down)\n","\n","        return E\n","class L_exp(nn.Module):\n","\n","    def __init__(self,patch_size,mean_val):\n","        super(L_exp, self).__init__()\n","        # print(1)\n","        self.pool = nn.AvgPool2d(patch_size)\n","        self.mean_val = mean_val\n","    def forward(self, x ):\n","\n","        b,c,h,w = x.shape\n","        x = torch.mean(x,1,keepdim=True)\n","        mean = self.pool(x)\n","\n","        d = torch.mean(torch.pow(mean- torch.FloatTensor([self.mean_val] ).cuda(),2))\n","        return d\n","        \n","class L_TV(nn.Module):\n","    def __init__(self,TVLoss_weight=1):\n","        super(L_TV,self).__init__()\n","        self.TVLoss_weight = TVLoss_weight\n","\n","    def forward(self,x):\n","        batch_size = x.size()[0]\n","        h_x = x.size()[2]\n","        w_x = x.size()[3]\n","        count_h =  (x.size()[2]-1) * x.size()[3]\n","        count_w = x.size()[2] * (x.size()[3] - 1)\n","        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n","        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n","        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n","class Sa_Loss(nn.Module):\n","    def __init__(self):\n","        super(Sa_Loss, self).__init__()\n","        # print(1)\n","    def forward(self, x ):\n","        # self.grad = np.ones(x.shape,dtype=np.float32)\n","        b,c,h,w = x.shape\n","        # x_de = x.cuda().detach().numpy()\n","        r,g,b = torch.split(x , 1, dim=1)\n","        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n","        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n","        Dr = r-mr\n","        Dg = g-mg\n","        Db = b-mb\n","        k =torch.pow( torch.pow(Dr,2) + torch.pow(Db,2) + torch.pow(Dg,2),0.5)\n","        # print(k)\n","        \n","\n","        k = torch.mean(k)\n","        return k\n","\n","class perception_loss(nn.Module):\n","    def __init__(self):\n","        super(perception_loss, self).__init__()\n","        features = vgg16(pretrained=True).features\n","        self.to_relu_1_2 = nn.Sequential() \n","        self.to_relu_2_2 = nn.Sequential() \n","        self.to_relu_3_3 = nn.Sequential()\n","        self.to_relu_4_3 = nn.Sequential()\n","\n","        for x in range(4):\n","            self.to_relu_1_2.add_module(str(x), features[x])\n","        for x in range(4, 9):\n","            self.to_relu_2_2.add_module(str(x), features[x])\n","        for x in range(9, 16):\n","            self.to_relu_3_3.add_module(str(x), features[x])\n","        for x in range(16, 23):\n","            self.to_relu_4_3.add_module(str(x), features[x])\n","        \n","        # don't need the gradients, just want the features\n","        for param in self.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, x):\n","        h = self.to_relu_1_2(x)\n","        h_relu_1_2 = h\n","        h = self.to_relu_2_2(h)\n","        h_relu_2_2 = h\n","        h = self.to_relu_3_3(h)\n","        h_relu_3_3 = h\n","        h = self.to_relu_4_3(h)\n","        h_relu_4_3 = h\n","        # out = (h_relu_1_2, h_relu_2_2, h_relu_3_3, h_relu_4_3)\n","        return h_relu_4_3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-26T08:28:49.593848Z","iopub.status.busy":"2022-01-26T08:28:49.593267Z","iopub.status.idle":"2022-01-26T09:55:07.13868Z","shell.execute_reply":"2022-01-26T09:55:07.137541Z","shell.execute_reply.started":"2022-01-26T08:28:49.593775Z"},"trusted":true},"outputs":[],"source":["#low_light train\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torch.backends.cudnn as cudnn\n","import torch.optim\n","import os\n","import sys\n","import argparse\n","import time\n","# import dataloader\n","# import model\n","# import Myloss\n","import numpy as np\n","from torchvision import transforms\n","\n","lowlight_images_path = '../input/low-light/data/train_data/'\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","\n","\n","def train(config):\n","\n","    os.environ['cuda_VISIBLE_DEVICES'] = '0'\n","\n","    DCE_net = enhance_net_nopool()\n","    if torch.cuda.is_available():\n","        DCE_net.cuda()\n","    DCE_net.apply(weights_init)\n","    if config.load_pretrain == True:\n","        DCE_net.load_state_dict(torch.load(config.pretrain_dir))\n","    train_dataset = lowlight_loader(lowlight_images_path)\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size = config.train_batch_size, shuffle=True, num_workers=config.num_workers, pin_memory=True)\n","\n","    l_color = L_color()\n","    l_spa = L_spa()\n","\n","    l_exp = L_exp(16, 0.6)\n","    l_TV = L_TV()\n","\n","    optimizer = torch.optim.Adam(DCE_net.parameters(\n","    ), lr=config.lr, weight_decay=config.weight_decay)\n","\n","    DCE_net.train()\n","\n","    for epoch in range(config.num_epochs):\n","        for iteration, img_lowlight in enumerate(train_loader):\n","\n","            img_lowlight = img_lowlight.cuda()\n","\n","            enhanced_image_1, enhanced_image, A = DCE_net(img_lowlight)\n","\n","            Loss_TV = 200*(l_TV(A))\n","\n","            loss_spa = torch.mean(l_spa(enhanced_image, img_lowlight))\n","\n","            loss_col = 5*torch.mean(l_color(enhanced_image))\n","\n","            loss_exp = 10*torch.mean(l_exp(enhanced_image))\n","\n","            # best_loss\n","            loss = Loss_TV + loss_spa + loss_col + loss_exp\n","            #\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(\n","                DCE_net.parameters(), config.grad_clip_norm)\n","            optimizer.step()\n","#             if ((iteration+1) % config.display_iter) == 0:\n","#                 print(\"Loss at iteration\", iteration+1, \":\", loss.item())\n","            if ((iteration+1) % config.snapshot_iter) == 0:\n","\n","                torch.save(DCE_net.state_dict(\n","                ), config.snapshots_folder + \"Epoch\" + str(epoch) + '.pth')\n","        print(f'Epoch : {epoch + 1}')\n","\n","\n","                \n","if __name__ == \"__main__\":\n","\n","    parser = argparse.ArgumentParser()\n","\n","        # Input Parameters\n","    parser.add_argument('-f')\n","    parser.add_argument('--lowlight_images_path', type=str,\n","                            default=\"data/train_data/\")\n","    parser.add_argument('--lr', type=float, default=0.0001)\n","    parser.add_argument('--weight_decay', type=float, default=0.0001)\n","    parser.add_argument('--grad_clip_norm', type=float, default=0.1)\n","    parser.add_argument('--num_epochs', type=int, default=200)#200\n","    parser.add_argument('--train_batch_size', type=int, default=8)#8\n","    #parser.add_argument('--val_batch_size', type=int, default=4)\n","    parser.add_argument('--num_workers', type=int, default=2)#4\n","    parser.add_argument('--display_iter', type=int, default=10)\n","    parser.add_argument('--snapshot_iter', type=int, default=10)\n","    parser.add_argument('--snapshots_folder', type=str, default=\"snapshots/\")\n","    parser.add_argument('--load_pretrain', type=bool, default=False)\n","    parser.add_argument('--pretrain_dir', type=str,default=\"snapshots/Epoch99.pth\")\n","\n","    config = parser.parse_args()\n","\n","    if not os.path.exists(config.snapshots_folder):\n","        os.mkdir(config.snapshots_folder)\n","\n","    train(config)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-26T09:55:07.142522Z","iopub.status.busy":"2022-01-26T09:55:07.141647Z","iopub.status.idle":"2022-01-26T09:56:25.268827Z","shell.execute_reply":"2022-01-26T09:56:25.267748Z","shell.execute_reply.started":"2022-01-26T09:55:07.142467Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torch.backends.cudnn as cudnn\n","import torch.optim\n","import os\n","import sys\n","import argparse\n","import time\n","# import dataloader\n","# import model\n","import numpy as np\n","from torchvision import transforms\n","from PIL import Image\n","import glob\n","import time\n","import cv2\n","\n","def apply_mask(matrix, mask, fill_value):\n","    masked = np.ma.array(matrix, mask=mask, fill_value=fill_value)\n","    return masked.filled()\n","\n","def apply_threshold(matrix, low_value, high_value):\n","    low_mask = matrix < low_value\n","    matrix = apply_mask(matrix, low_mask, low_value)\n","\n","    high_mask = matrix > high_value\n","    matrix = apply_mask(matrix, high_mask, high_value)\n","\n","    return matrix\n","\n","def SCB_RGB(img, percent):\n","    assert img.shape[2] == 3\n","    assert percent > 0 and percent < 100\n","\n","    half_percent = percent / 200.0\n","\n","    channels = cv2.split(img)\n","\n","    out_channels = []\n","    for channel in channels:\n","        assert len(channel.shape) == 2\n","        # find the low and high precentile values (based on the input percentile)\n","        height, width = channel.shape\n","        vec_size = width * height\n","        flat = channel.reshape(vec_size)\n","\n","        assert len(flat.shape) == 1\n","\n","        flat = np.sort(flat)\n","\n","        n_cols = flat.shape[0]\n","\n","        low_val  = flat[math.floor(n_cols * half_percent)]\n","        high_val = flat[math.ceil( n_cols * (1.0 - half_percent))]\n","\n","        # saturate below the low percentile and above the high percentile\n","        thresholded = apply_threshold(channel, low_val, high_val)\n","        # scale the channel\n","        normalized = cv2.normalize(thresholded, thresholded.copy(), 0, 255, cv2.NORM_MINMAX)\n","        out_channels.append(normalized)\n","\n","    return cv2.merge(out_channels)\n","\n","def lowlight(image_path):\n","\tos.environ['CUDA_VISIBLE_DEVICES']='0'\n","\tdata_lowlight = Image.open(image_path)\n","\n","\n","\tdata_lowlight = (np.asarray(data_lowlight)/255.0)\n","\n","\n","\tdata_lowlight = torch.from_numpy(data_lowlight).float()\n","\tdata_lowlight = data_lowlight.permute(2,0,1)\n","\tdata_lowlight = data_lowlight.cuda().unsqueeze(0)\n","\n","\tDCE_net = enhance_net_nopool().cuda()\n","\tDCE_net.load_state_dict(torch.load('snapshots/Epoch0.pth',map_location = 'cuda'))\n","\tstart = time.time()\n","\t_,enhanced_image,_ = DCE_net(data_lowlight)\n","\tend_time = (time.time() - start)\n","\tprint(end_time)\n","\timage_path = image_path.replace('test_data','result')\n","\tresult_path = image_path\n","\t# if not os.path.exists(image_path.replace('/'+image_path.split(\"/\")[-1],'')):\n","\t# \tos.makedirs(image_path.replace('/'+image_path.split(\"/\")[-1],''))\n","\n","\ttorchvision.utils.save_image(enhanced_image, result_path)\n","\n","if __name__ == '__main__':\n","# test_images\n","\twith torch.no_grad():\n","\t\tfilePath = '/tmp/test_data/'\n","\t\n","\t\tfile_list = os.listdir(filePath)\n","\n","\t\tfor file_name in file_list:\n","\t\t\ttest_list = glob.glob(filePath+file_name+\"/*\") \n","\t\t\tfor image in test_list:\n","\t\t\t\t# image = image\n","\t\t\t\tprint(image)\n","\t\t\t\tlowlight(image)\n","\t\t\t\tscb_image = SCB_RGB(cv2.imread(image.replace('test_data','result')),5)\n","\t\t\t\tcv2.imwrite(image.replace('test_data','after_scb'),scb_image)\n","\n","\t\t\t\tdst = cv2.fastNlMeansDenoisingColored(cv2.imread(image.replace('test_data','after_scb')), None, 5, 5, 3, 15)\n","\t\t\t\tcv2.imwrite(image.replace('test_data','after_nr'),dst)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-26T09:56:25.276186Z","iopub.status.busy":"2022-01-26T09:56:25.274479Z","iopub.status.idle":"2022-01-26T09:57:44.536715Z","shell.execute_reply":"2022-01-26T09:57:44.535767Z","shell.execute_reply.started":"2022-01-26T09:56:25.276123Z"},"trusted":true},"outputs":[],"source":["filePath = '/tmp/test_data/'\n","\t\n","file_list = os.listdir(filePath)\n","\n","for file_name in file_list:\n","    test_list = glob.glob(filePath+file_name+\"/*\") \n","    for image in test_list:\n","        plt.subplots(figsize = (24,12))\n","        plt.subplot(2,3,1)\n","        plt.imshow(Image.open(image))\n","        plt.title('Original Image')\n","        plt.xticks([]),plt.yticks([])\n","        plt.subplot(2,3,2)\n","        plt.imshow(Image.open(image.replace('test_data','result')))\n","        plt.title('CLAHE + Zero-DCE output')\n","        plt.xticks([]),plt.yticks([])\n","        plt.subplot(2,3,3)\n","        plt.imshow(Image.open(image.replace('test_data','after_nr')))\n","        plt.title('After SCB + NR Output')\n","        plt.xticks([]),plt.yticks([])\n","        hist = cv2.calcHist(cv2.imread(image,0),[0], None , [256], [0,256])\n","        hist_dce = cv2.calcHist(cv2.imread(image.replace('test_data','result'),0),[0], None , [256], [0,256])\n","        hist_NR = cv2.calcHist(cv2.imread(image.replace('test_data','after_nr'),0),[0], None , [256], [0,256])\n","        plt.subplot(2,3,4)\n","        plt.plot(hist)\n","        plt.subplot(2,3,5)\n","        plt.plot(hist_dce)\n","        plt.subplot(2,3,6)\n","        plt.plot(hist_NR)\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-26T09:57:44.539123Z","iopub.status.busy":"2022-01-26T09:57:44.538489Z","iopub.status.idle":"2022-01-26T09:57:44.543458Z","shell.execute_reply":"2022-01-26T09:57:44.54263Z","shell.execute_reply.started":"2022-01-26T09:57:44.53907Z"},"trusted":true},"outputs":[],"source":["# !cp -r '../input/low-light/data/result' /tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-26T09:57:44.546088Z","iopub.status.busy":"2022-01-26T09:57:44.545346Z","iopub.status.idle":"2022-01-26T09:57:44.561088Z","shell.execute_reply":"2022-01-26T09:57:44.560045Z","shell.execute_reply.started":"2022-01-26T09:57:44.546041Z"},"trusted":true},"outputs":[],"source":["# !cp -r '../input/low-light/data/test_data' /tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-26T09:57:44.563556Z","iopub.status.busy":"2022-01-26T09:57:44.563097Z","iopub.status.idle":"2022-01-26T09:57:44.571638Z","shell.execute_reply":"2022-01-26T09:57:44.570582Z","shell.execute_reply.started":"2022-01-26T09:57:44.563509Z"},"trusted":true},"outputs":[],"source":["# !cp -r '../input/low-light/data/after_scb' /tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-26T09:57:44.576203Z","iopub.status.busy":"2022-01-26T09:57:44.575951Z","iopub.status.idle":"2022-01-26T09:57:44.581001Z","shell.execute_reply":"2022-01-26T09:57:44.579652Z","shell.execute_reply.started":"2022-01-26T09:57:44.576172Z"},"trusted":true},"outputs":[],"source":["# !cp -r '../input/low-light/data/after_nr' /tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
